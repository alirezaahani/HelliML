{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "g1mEqlXyVG0Z"
      },
      "outputs": [],
      "source": [
        "# @title Load libraries\n",
        "!pip install -U transformers datasets huggingface_hub accelerate transformers[torch] evaluate --quiet\n",
        "\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset, ClassLabel, load_from_disk, load_metric\n",
        "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, AutoTokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import accelerate\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVsC5Gbzat0c"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xy9MqlP34zc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPsZGCwfoWFZ"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Loader\n",
        "\n",
        "load_from_cache = True # @param {type: \"boolean\"}\n",
        "local_cache_path = '/content/drive/MyDrive/Spam detection/Dataset'  # @param {type: \"string\"}\n",
        "\n",
        "all_datasets = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70bU97HmVc_O"
      },
      "outputs": [],
      "source": [
        "def clean(batch):\n",
        "  batch['text'] = [' '.join(text.split()) if text else '' for text in batch['text']]\n",
        "  return batch\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "def adjust_labels(batch):\n",
        "  batch['label_'] = [0 if label == 'ham' else 1 for label in batch['label']]\n",
        "  return batch\n",
        "\n",
        "def str_int_labels(batch):\n",
        "  batch['label_'] = int(batch['label'])\n",
        "  return batch\n",
        "\n",
        "def all_ham(batch):\n",
        "  batch['label'] = 0\n",
        "  return batch\n",
        "\n",
        "def all_spam(batch):\n",
        "  batch['label'] = 1\n",
        "  return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4OnA2psoaLf"
      },
      "outputs": [],
      "source": [
        "if load_from_cache:\n",
        "  all_datasets = load_from_disk(local_cache_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4d5PyU5oa39"
      },
      "outputs": [],
      "source": [
        "if not load_from_cache:\n",
        "\n",
        "  # Spam & Ham mixed\n",
        "  scam_spam = (\n",
        "      load_dataset(\"FredZhang7/all-scam-spam\")[\"train\"]\n",
        "      .rename_column(\"is_spam\", \"label\")\n",
        "  )\n",
        "  sms_spam = (\n",
        "      load_dataset(\"sms_spam\")[\"train\"]\n",
        "      .rename_column(\"sms\", \"text\")\n",
        "      .map(adjust_labels, batched=True)\n",
        "      .remove_columns(['label'])\n",
        "      .rename_column('label_', 'label')\n",
        "  )\n",
        "  spam_messages = (\n",
        "      load_dataset(\n",
        "          \"mshenoda/spam-messages\",\n",
        "          data_files=[\n",
        "              \"spam_messages_test.csv\",\n",
        "              \"spam_messages_val.csv\",\n",
        "              \"spam_messages_train.csv\",\n",
        "          ],\n",
        "      )[\"train\"]\n",
        "      .map(adjust_labels, batched=True)\n",
        "      .remove_columns(['label'])\n",
        "      .rename_column('label_', 'label')\n",
        "  )\n",
        "  email_spam = (\n",
        "      load_dataset(\"NotShrirang/email-spam-filter\")[\"train\"]\n",
        "      .remove_columns([\"Unnamed: 0\", \"label\"])\n",
        "      .rename_column(\"label_num\", \"label\")\n",
        "  )\n",
        "  enron_spam = (\n",
        "      load_dataset(\"SetFit/enron_spam\")\n",
        "      .remove_columns([\"message_id\", \"label_text\", \"subject\", \"message\", \"date\"])\n",
        "  )\n",
        "  enron_spam = concatenate_datasets([enron_spam[\"train\"], enron_spam[\"test\"]])\n",
        "\n",
        "  misinformation = (\n",
        "      load_dataset(\"daviddaubner/misinformation-detection\")\n",
        "  )\n",
        "\n",
        "  misinformation = concatenate_datasets([misinformation[\"train\"], misinformation[\"test\"], misinformation[\"validation\"]])\n",
        "\n",
        "  # All spam\n",
        "  Health_Misinfo = (\n",
        "      load_dataset(\"TheoTsio/Health_Misinfo\")[\"train\"]\n",
        "      .remove_columns(\n",
        "          [\"Timestamp\", \"Url\", \"Domain\", \"Num_Emoji\", \"Num_Bad_Words\", \"Credibility\"]\n",
        "      )\n",
        "      .rename_column(\"Document\", \"text\")\n",
        "      .map(all_spam)\n",
        "  )\n",
        "\n",
        "  political_news_justifications = (\n",
        "      load_dataset(\"od21wk/political_news_justifications\")['train']\n",
        "      .remove_columns(['completion'])\n",
        "      .rename_column('prompt', 'text')\n",
        "      .map(all_spam)\n",
        "  )\n",
        "\n",
        "  advertisementText = (\n",
        "      load_dataset(\"Chinxian1121/advertisementText\")['train']\n",
        "      .map(all_spam)\n",
        "  )\n",
        "\n",
        "  advertisement_copy = (\n",
        "      load_dataset(\"jaykin01/advertisement-copy\")['train']\n",
        "      .remove_columns(\n",
        "          [\"product\", \"description\", \"Unnamed: 3\"]\n",
        "      )\n",
        "      .rename_column('ad', 'text')\n",
        "      .map(all_spam)\n",
        "  )\n",
        "\n",
        "  '''persian_blog = (\n",
        "      load_dataset(\"RohanAiLab/persian_blog\", split=\"train[:30%]\")\n",
        "      .map(all_ham)\n",
        "  )\n",
        "  custom_dataset = (\n",
        "      load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/Datasets/spam_persian.csv\")['train']\n",
        "      .rename_column(' label', 'label')\n",
        "      .map(all_spam)\n",
        "  )\n",
        "  '''\n",
        "\n",
        "  clickbait_notclickbait_dataset = concatenate_datasets([\n",
        "      load_dataset(\"christinacdl/clickbait_notclickbait_dataset\", split='train'),\n",
        "      load_dataset(\"christinacdl/clickbait_notclickbait_dataset\", split='test'),\n",
        "      load_dataset(\"christinacdl/clickbait_notclickbait_dataset\", split='validation')\n",
        "  ])\n",
        "\n",
        "  twitter_misinformation = load_dataset(\"roupenminassian/twitter-misinformation\")\n",
        "  twitter_misinformation = (\n",
        "      concatenate_datasets([twitter_misinformation['train'], twitter_misinformation['test']])\n",
        "      .remove_columns(['Unnamed: 0.1', 'Unnamed: 0'])\n",
        "  )\n",
        "  '''persian_blog,\n",
        "  custom_dataset,'''\n",
        "\n",
        "  all_datasets = concatenate_datasets([\n",
        "      scam_spam,\n",
        "      sms_spam,\n",
        "      spam_messages,\n",
        "      email_spam,\n",
        "      enron_spam,\n",
        "      Health_Misinfo,\n",
        "      political_news_justifications,\n",
        "      misinformation,\n",
        "      advertisementText,\n",
        "      advertisement_copy,\n",
        "      clickbait_notclickbait_dataset,\n",
        "      twitter_misinformation,\n",
        "  ]).shuffle(seed=49).train_test_split(test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR92xEbHi_Xb"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   load_accuracy = evaluate.load(\"accuracy\")\n",
        "   load_f1 = evaluate.load(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "\n",
        "   return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owsMZ0WBjK9i"
      },
      "outputs": [],
      "source": [
        "train = all_datasets['train']\n",
        "test = all_datasets['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnmizCucwfi2"
      },
      "outputs": [],
      "source": [
        "print(f\"Size of train dataset: {len(train)}\")\n",
        "print(f\"Size of test dataset: {len(test)}\")\n",
        "\n",
        "print(train)\n",
        "print(test)\n",
        "\n",
        "print(train[0])\n",
        "print(test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv0azoN_Sz_I"
      },
      "outputs": [],
      "source": [
        "for dataset, name in zip((train, test), ('Train', 'Test')):\n",
        "  plt.figure()\n",
        "  df = dataset.to_pandas()\n",
        "\n",
        "  df['label'].value_counts().plot(kind='bar', label=name)\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.xlabel('Class')\n",
        "  plt.ylabel('Count')\n",
        "  plt.xticks([0, 1], ['Ham', 'Spam'], rotation=0)  # The label 0 is for 'Spam' and 1 is for 'Ham'\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvt3bCtnjFjD"
      },
      "outputs": [],
      "source": [
        "# @title Training and evaluation\n",
        "\n",
        "train_model = True # @param {type: \"boolean\"}\n",
        "evaluate_model = True # @param {type: \"boolean\"}\n",
        "load_from_disk = True # @param {type: \"boolean\"}\n",
        "\n",
        "print_layers = True # @param {type: \"boolean\"}\n",
        "\n",
        "\n",
        "model_name = \"xlm-roberta-base\" # @param {type: \"string\"}\n",
        "output_path = \"/content/drive/MyDrive/Spam detection/Model\" # @param {type: \"string\"}\n",
        "freeze_layers_until = \"roberta.encoder.layer.11\" # @param {type: \"string\"}\n",
        "learning_rate = 1e-4 # @param {type: \"number\"}\n",
        "num_train_epochs = 1 # @param {type: \"number\"}\n",
        "weight_decay = 0.001 # @param {type: \"number\"}\n",
        "per_device_train_batch_size = 128 # @param {type: \"number\"}\n",
        "max_steps = -1 # @param {type: \"number\"}\n",
        "fp16 = True # @param {type: \"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFt4HMu2oD7-"
      },
      "outputs": [],
      "source": [
        "if load_from_disk:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(output_path)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(output_path, num_labels=2)\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojFA7VivoGrT"
      },
      "outputs": [],
      "source": [
        "if not load_from_cache:\n",
        "  all_datasets = all_datasets.map(clean, batched=True).map(tokenize, batched=True)\n",
        "\n",
        "  train = all_datasets['train']\n",
        "  test = all_datasets['test']\n",
        "\n",
        "  all_datasets.save_to_disk(local_cache_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlxrbOaIoJer"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(freeze_layers_until):\n",
        "    break\n",
        "  param.requires_grad = False\n",
        "\n",
        "if print_layers:\n",
        "  for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdegIk7aoNOT"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "   output_dir=output_path,\n",
        "   learning_rate=learning_rate,\n",
        "   num_train_epochs=num_train_epochs,\n",
        "   weight_decay=weight_decay,\n",
        "   save_strategy=\"epoch\",\n",
        "   evaluation_strategy=\"steps\",\n",
        "   eval_steps=600,\n",
        "   fp16=fp16,\n",
        "   logging_steps=200,\n",
        "   per_device_train_batch_size=per_device_train_batch_size,\n",
        "   per_device_eval_batch_size=per_device_train_batch_size,\n",
        "   dataloader_num_workers=2,\n",
        "   max_steps=max_steps\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=train,\n",
        "   eval_dataset=test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPGP8p10oPwK"
      },
      "outputs": [],
      "source": [
        "if train_model:\n",
        "  trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUL3s-IAoQni"
      },
      "outputs": [],
      "source": [
        "if evaluate_model:\n",
        "  print(trainer.evaluate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icy9WRrEt3D4"
      },
      "outputs": [],
      "source": [
        "if evaluate_model:\n",
        "  import torch\n",
        "\n",
        "  examples = [\n",
        "      {'text': 'این تست متن فارسی غیر اسپم است.', 'label': 0},\n",
        "      {'text': 'خرید هاست لینوکس ایران', 'label': 1},\n",
        "      {'text': 'ترجمه تخصصی و فنی به آلمانی: رویکردها و استراتژی‌ها', 'label': 0},\n",
        "      {'text': 'فایلهای پرکاربرد و آموزشی و با کیفیت برای استفاده دانشجویان وتحقیقات علمی و پژوهشی', 'label': 1},\n",
        "      {'text': 'فرصت‌های تحصیل رایگان در اروپا: بورس‌های تحصیلی به عنوان پلی به دسترسی به تعلیمات برتر', 'label': 1},\n",
        "      {'text': 'امروز ساعت 6:45 از خواب بیدار شدم و از همون لحظه حوصله هیچکس رو ندارم :|', 'label': 0},\n",
        "      {'text': '''اَلا یا اَیُّهَا السّاقی اَدِرْ کَأسَاً و ناوِلْها که عشق آسان نمود اوّل ولی افتاد مشکل‌ه''', 'label': 0},\n",
        "      {'text': 'سخنگوی شورای امنیت ملی کاخ سفید گفت که این کشور به ایران پیام داده که نمی‌خواهد شاهد گسترش درگیری در منطقه باشد.', 'label': 0},\n",
        "      {'text': 'سوالات استخدامی علوم پزشکی و بیمارستانها 1402 ,سوالات استخدامی وزارت بهداشت+سوالات استخدامی بیمارستان 1402- نمونه سوالات استخدامی رایگان,', 'label': 1},\n",
        "      {'text': ' روند رانندگی بی صدا، بدون سر و صدای غیر عادی، با ظاهری بسیارشیک و جمع و جور است و دید راننده را محدود نمی کند. توربین این دستگاه در طول روز برای پخش عود می چرخد ترکیبی قوی از آلیاژ با مقاومت بالا و سرامیک طبیعی، بدون ترس از قرار گرفتن در معرض آفتاب و در تابستان بسیار قوی کار میکند.قیمت این محصول...تومان', 'label': 1},\n",
        "      {'text': 'DeciLM-7B: The Fastest and Most Accurate 7B-Parameter LLM to Date', 'label': 0},\n",
        "      {'text': 'Telecom Industry Is Mad Because the FCC Might Examine High Broadband Prices', 'label': 0},\n",
        "      {'text': 'Well, here is hope that this will be a first step in bringing US internet access to at least something comparable to Balkans. ', 'label': 0},\n",
        "      {'text': 'Agree to notifications to allow news feed', 'label': 1},\n",
        "      {'text': '6 Ways to Boost Your Coffee with Vitamins and Antioxidants', 'label': 1}\n",
        "  ]\n",
        "\n",
        "  for example in examples:\n",
        "    print('*' * 40)\n",
        "    print('Text:', example['text'], '\\n')\n",
        "\n",
        "    inputs = tokenizer(example['text'], return_tensors=\"pt\")\n",
        "    inputs = {name: tensor.to('cuda') for name, tensor in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(**inputs).logits\n",
        "\n",
        "    print('Ref:', {0: 'Ham', 1: 'Spam'}[example['label']])\n",
        "    print('Model:', {0: 'Ham', 1: 'Spam'}[logits.argmax().item()])\n",
        "    print('Ham confidence:', logits.softmax(-1)[0][0].item())\n",
        "    print('Spam confidence:', logits.softmax(-1)[0][1].item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "nBI1kjrBJir6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os"
      ],
      "metadata": {
        "id": "ozDKAES8JpKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "HgWV6uJjJsUn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}